{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c5eb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235d730c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Local PySpark ETL\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ee3277",
   "metadata": {},
   "source": [
    "### Extracting data from landing_zone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1dabd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orders = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"encoding\", \"UTF-8\") \\\n",
    "    .load(\"data/landing_zone/orders/19980505/orders_initial.csv\")\n",
    "\n",
    "df_categories = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"encoding\", \"UTF-8\") \\\n",
    "    .load(\"data/landing_zone/categories/categories.csv\")\n",
    "\n",
    "df_customers = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"encoding\", \"UTF-8\") \\\n",
    "    .load(\"data/landing_zone/customers/customers.csv\")\n",
    "\n",
    "df_orders_details = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"encoding\", \"UTF-8\") \\\n",
    "    .load(\"data/landing_zone/orders_details/orders_details.csv\")\n",
    "\n",
    "df_products = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"encoding\", \"UTF-8\") \\\n",
    "    .load(\"data/landing_zone/products/products.csv\")\n",
    "\n",
    "df_suppliers = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"encoding\", \"UTF-8\") \\\n",
    "    .load(\"data/landing_zone/suppliers/suppliers.csv\")\n",
    "\n",
    "df_products.show(5)\n",
    "df_products.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50300006",
   "metadata": {},
   "source": [
    "### RAW ZONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b4d3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns for join\n",
    "df_orders_raw = df_orders.withColumnRenamed(\"orderid\", \"order_id\")\n",
    "df_orders_details_raw = df_orders_details.withColumnRenamed(\"orderid\", \"order_id\")\n",
    "df_products_raw = df_products.withColumnRenamed(\"productid\", \"product_id\")\n",
    "df_categories_raw = df_categories.withColumnRenamed(\"categoryid\", \"category_id\")\n",
    "df_customers_raw = df_customers.withColumnRenamed(\"customerid\", \"customer_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9e18bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, to_date\n",
    "from pyspark.sql.types import IntegerType, DoubleType\n",
    "\n",
    "df_orders_raw = df_orders_raw \\\n",
    "    .withColumn(\"order_id\", col(\"order_id\").cast(IntegerType())) \\\n",
    "    .withColumn(\"employeeid\", col(\"employeeid\").cast(IntegerType())) \\\n",
    "    .withColumn(\"orderdate\", to_date(\"orderdate\", \"yyyy-MM-dd\")) \\\n",
    "    .withColumn(\"requireddate\", to_date(\"requireddate\", \"yyyy-MM-dd\")) \\\n",
    "    .withColumn(\"shippeddate\", to_date(\"shippeddate\", \"yyyy-MM-dd\"))\n",
    "\n",
    "df_orders_details_raw = df_orders_details_raw \\\n",
    "    .withColumn(\"productid\", col(\"productid\").cast(IntegerType())) \\\n",
    "    .withColumn(\"unitprice\", col(\"unitprice\").cast(DoubleType())) \\\n",
    "    .withColumn(\"quantity\", col(\"quantity\").cast(IntegerType())) \\\n",
    "    .withColumn(\"discount\", col(\"discount\").cast(DoubleType()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78995970",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "raw_orders_path = r\"D:\\Study-By_Myself-Knowledge\\PySpark_pro\\AWS_PySpark_Workshop\\local\\data-pipeline-with-PySpark\\data\\raw_zone\\orders_raw\"\n",
    "raw_order_details_path = r\"D:\\Study-By_Myself-Knowledge\\PySpark_pro\\AWS_PySpark_Workshop\\local\\data-pipeline-with-PySpark\\data\\raw_zone\\orders_details_raw\"\n",
    "\n",
    "os.makedirs(raw_orders_path, exist_ok=True)\n",
    "os.makedirs(raw_order_details_path, exist_ok=True)\n",
    "\n",
    "# Spark -> Pandas\n",
    "orders_pdf = df_orders_raw.toPandas()\n",
    "order_details_pdf = df_orders_details_raw.toPandas()\n",
    "\n",
    "orders_pdf.to_csv(\n",
    "    os.path.join(raw_orders_path, \"orders_raw.csv\"),\n",
    "    index=False,\n",
    "    encoding=\"utf-8-sig\"\n",
    ")\n",
    "order_details_pdf.to_csv(\n",
    "    os.path.join(raw_order_details_path, \"orders_details_raw.csv\"),\n",
    "    index=False,\n",
    "    encoding=\"utf-8-sig\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0bca65c",
   "metadata": {},
   "source": [
    "### SERVING ZONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f9ce59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# join orders + order_details\n",
    "df_fact_orders_items = df_orders_raw.join(\n",
    "    df_orders_details_raw,\n",
    "    on=\"order_id\",\n",
    "    how=\"inner\"\n",
    ")\n",
    "df_fact_orders_items.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8e324d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import year, month, dayofmonth\n",
    "\n",
    "df_fact_orders_items = df_fact_orders_items \\\n",
    "    .withColumn(\"year\", year(\"orderdate\")) \\\n",
    "    .withColumn(\"month\", month(\"orderdate\")) \\\n",
    "    .withColumn(\"day\", dayofmonth(\"orderdate\")) \\\n",
    "    .withColumn(\"year_be\", year(\"orderdate\") + 543)\n",
    "df_fact_orders_items.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90172c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fact_orders_items.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a808f931",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import IntegerType, DoubleType, DateType\n",
    "\n",
    "df_serving = df_fact_orders_items \\\n",
    "    .withColumn(\"order_id\", col(\"order_id\").cast(IntegerType())) \\\n",
    "    .withColumn(\"employeeid\", col(\"employeeid\").cast(IntegerType())) \\\n",
    "    .withColumn(\"shipvia\", col(\"shipvia\").cast(IntegerType())) \\\n",
    "    .withColumn(\"freight\", col(\"freight\").cast(DoubleType())) \\\n",
    "    .withColumn(\"productid\", col(\"productid\").cast(IntegerType())) \\\n",
    "    .withColumn(\"unitprice\", col(\"unitprice\").cast(DoubleType())) \\\n",
    "    .withColumn(\"quantity\", col(\"quantity\").cast(IntegerType())) \\\n",
    "    .withColumn(\"discount\", col(\"discount\").cast(DoubleType()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17434cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add Sales Metrics\n",
    "df_serving = df_serving \\\n",
    "    .withColumn(\"gross_sales\", col(\"unitprice\") * col(\"quantity\")) \\\n",
    "    .withColumn(\n",
    "        \"net_sales\",\n",
    "        col(\"unitprice\") * col(\"quantity\") * (1 - col(\"discount\"))\n",
    "    ) \\\n",
    "    .withColumn(\n",
    "        \"discount_amount\",\n",
    "        col(\"unitprice\") * col(\"quantity\") * col(\"discount\")\n",
    "    )\n",
    "df_serving.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b0d7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when\n",
    "df_serving = df_serving.withColumn(\n",
    "    \"has_discount\",\n",
    "    when(col(\"discount\") > 0, \"Y\").otherwise(\"N\")\n",
    ")\n",
    "df_serving.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845ccbfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shipping Intelligence\n",
    "df_serving = df_serving.withColumn(\n",
    "    \"shipping_type\",\n",
    "    when(col(\"freight\") > 50, \"HIGH_COST\")\n",
    "    .when(col(\"freight\") > 20, \"MEDIUM_COST\")\n",
    "    .otherwise(\"LOW_COST\")\n",
    ")\n",
    "df_serving.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b2b9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_serving= df_serving.withColumn(\n",
    "    \"order_size\",\n",
    "    when(col(\"quantity\") < 10, \"SMALL\")\n",
    "    .when(col(\"quantity\") < 30, \"MEDIUM\")\n",
    "    .otherwise(\"LARGE\")\n",
    ")\n",
    "df_serving.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ea26c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, date_format, to_date\n",
    "\n",
    "df_serving = df_serving.withColumn(\n",
    "    \"order_date\",\n",
    "    to_date(col(\"orderdate\"))\n",
    ").withColumn(\n",
    "    \"month_name\",\n",
    "    date_format(col(\"order_date\"), \"MMM\")\n",
    ")\n",
    "df_serving.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5c7540",
   "metadata": {},
   "source": [
    "### Export file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00ed6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_serving = df_serving.withColumn(\n",
    "    \"order_size\",\n",
    "    when(col(\"quantity\") < 10, \"SMALL\")\n",
    "    .when(col(\"quantity\") < 30, \"MEDIUM\")\n",
    "    .otherwise(\"LARGE\")\n",
    ")\n",
    "\n",
    "serving_folder = r\"D:\\Study-By_Myself-Knowledge\\PySpark_pro\\AWS_PySpark_Workshop\\local\\data-pipeline-with-PySpark\\data\\serving_zone\"\n",
    "os.makedirs(serving_folder, exist_ok=True)\n",
    "\n",
    "pdf = df_serving.toPandas()\n",
    "\n",
    "pdf.to_csv(\n",
    "    os.path.join(serving_folder, \"fact_sale_orders_items.csv\"),\n",
    "    index=False,\n",
    "    encoding=\"utf-8-sig\"\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
